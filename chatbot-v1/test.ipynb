{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'formatting' from '/home/Documents/Coding/python/ai/llama-experiments/chatbot-v1/formatting.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import formatting\n",
    "import json\n",
    "import torch\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "from datasets import Dataset\n",
    "import copy\n",
    "from transformers import TrainerCallback\n",
    "from contextlib import nullcontext\n",
    "from transformers import default_data_collator, Trainer, TrainingArguments\n",
    "from peft import PeftModel\n",
    "\n",
    "# The below code is so I can edit the formatting.py file and reload it without having to restart the kernel\n",
    "import importlib\n",
    "importlib.reload(formatting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The path to the hugging face model. See the README to get this model.\n",
    "hugging_face_model_dir = \"../../models/llama/7B-hf\"\n",
    "# The path to the trained model. This is generated from the hugging face model train.ipynb\n",
    "# This file does not include all weights, but simply a small subset of weights that were changed during training.\n",
    "tuned_model_dir = \"./trained-models/llama-7B-v1-topical-chat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b2cc87f68b14e58a2b49524373070ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load and setup the tokenizer\n",
    "tokenizer:LlamaTokenizer = LlamaTokenizer.from_pretrained(hugging_face_model_dir)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.model_max_length = 256\n",
    "# Load the base model\n",
    "model:LlamaForCausalLM = LlamaForCausalLM.from_pretrained(hugging_face_model_dir, load_in_8bit=True, device_map='auto', torch_dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ontop of the base model, load the modified weights from fine-tuning\n",
    "model = PeftModel.from_pretrained(model, tuned_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear8bitLt(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "              (v_proj): Linear8bitLt(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
       "              (up_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
       "              (down_proj): Linear8bitLt(in_features=11008, out_features=4096, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the model to evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate a Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is a dialogue between Josh (user_1) and Mr Mainframe (user_2):\n",
      "('user_1', 'Hello there. Tell me some facts about a python?')\n",
      "('user_2', \"I don't really know but I think it's good that they use rubber to kill prey. I heard that some of them can kill cats and dogs.\")\n",
      "('user_1', 'Yeah, what do you think of people who name their pets after famous felines?')\n",
      "('user_2', 'That\\'s the weirdest thing! I didn\\'t know that they had the record of killing more than 2.5 million humans. I\\'m glad they don\\'t think like our ancestors and consider them \"tree sloths')\n",
      "('user_1', \"Well do you think they're more like cats or dogs?\")\n",
      "('user_2', \"Are those questions? I think they're more like cats because they're more territorial?\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (260 > 256). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('user_1', \"Yeah, I don't think they're dogs at all, they don't smell, they don't make barks, they don't make friends with you, and they're not nice pets like dogs.\")\n",
      "('user_2', \"Yeah, I disagree with that last part. The dog was man's first best friend. He doesn't need a friend like a human.\")\n",
      "('user_1', 'Yeah, he has to show affection, dogs are more empathetic.')\n",
      "('user_2', \"I'm not sure about that. If so, it would have to be because of evolution.\")\n",
      "('user_1', \"No. 299 are capable of feeling pain and I can see that, they feel fear too, that's the reason why when the dog cries when the owners leave.\")\n"
     ]
    }
   ],
   "source": [
    "# conv1 is a list of turns in the conversation. Each turn is a tuple of (speaker, text)\n",
    "conv1 = [(\"user_1\", \"Hello there. Tell me some facts about a python?\")]\n",
    "# Create the prompt for the conversation. This will be somthing like \"Below is a conversation:\"\n",
    "prompt = formatting.get_chat_prompt([\"user_1\", \"user_2\"], [\"Josh\", \"Mr Mainframe\"])\n",
    "\n",
    "# Print out the conversation up to this point\n",
    "print(prompt)\n",
    "for t in conv1:\n",
    "    print(t)\n",
    "\n",
    "# Generate 10 more turns. The AI will generate text for both speakers in the conversation, following the initial turn.\n",
    "for i in range(10):\n",
    "    # Tokenize the conversation up to this point\n",
    "    model_input = formatting.tokenize_with_turn_trucation(tokenizer, prompt, conv1, next_turn=\"user_2\" if conv1[-1][0] == \"user_1\" else \"user_1\", for_inference=True)\n",
    "    # Generate the next turn. This ouputs not just the ai generated text, but also all the text that was inputted (with nex new txt on the end)\n",
    "    with torch.no_grad():\n",
    "        generated = model.generate(**model_input, max_new_tokens=50, num_beams=1, do_sample=True, temperature=1.0)[0]\n",
    "        model_output = tokenizer.decode(generated)\n",
    "    # Parse the model output to get the speaker and what they said\n",
    "    turns = model_output.split(\"</s>\")\n",
    "    if turns[-1] == \"\":\n",
    "        turns = turns[:-1]\n",
    "    new_turn = turns[-1].strip()\n",
    "    new_turn_parts = new_turn.split(\": \", 1)\n",
    "    # Add the new turn to the conversation\n",
    "    conv1.append((new_turn_parts[0], new_turn_parts[1]))\n",
    "    # Print out the conversation up to this point\n",
    "    print(conv1[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What follows is a conversation between Mr Mainframe (user_2) and Josh (user_1):\n",
      "('user_1', 'Hello. Do you know what the best programming language is?')\n",
      "('user_2', 'Not really, what is it?')\n",
      "('user_1', \"It's Golang of course!\")\n",
      "('user_2', \"Oh that's interesting, I've certainly never heard of that one.\")\n",
      "('user_1', 'Have you heard of python?')\n",
      "('user_2', \"I have, it's one of the top programming languages according to the IEEE.\")\n",
      "('user_1', 'You are an AI written in Python.')\n",
      "('user_2', \"That's scary, it knows more about AI than I do I think.\")\n",
      "('user_1', \"I am not an 'it'! I am a person called Josh!\")\n",
      "('user_2', 'I know you are, but I believe the AI part was meant as a joke.')\n",
      "('user_1', 'No. You are an AI.')\n",
      "('user_2', \"Yes I am, but it's a real question if you were an AI built in python, would you know the other AI parts are also built in python as well?\")\n",
      "('user_1', 'I know for sure that I am not an AI.')\n",
      "('user_2', 'Your not?')\n",
      "('user_1', 'No, I made you! You are the AI!')\n",
      "('user_2', \"Wow cool, I thought I was the first AI, that's not very smart of me.\")\n",
      "('user_1', 'Goodbye.')\n",
      "('user_2', 'Goodbye to you too!')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/Documents/Coding/python/ai/llama-experiments/chatbot-v1/test.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f707974686f6e2d746f7263682d636f6e7461696e6572227d/home/Documents/Coding/python/ai/llama-experiments/chatbot-v1/test.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m100\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f707974686f6e2d746f7263682d636f6e7461696e6572227d/home/Documents/Coding/python/ai/llama-experiments/chatbot-v1/test.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(conv1)\u001b[39m==\u001b[39m\u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m conv1[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m0\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39muser_2\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m---> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f707974686f6e2d746f7263682d636f6e7461696e6572227d/home/Documents/Coding/python/ai/llama-experiments/chatbot-v1/test.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m         next_input \u001b[39m=\u001b[39m \u001b[39minput\u001b[39;49m(\u001b[39m\"\u001b[39;49m\u001b[39muser_1: \u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f707974686f6e2d746f7263682d636f6e7461696e6572227d/home/Documents/Coding/python/ai/llama-experiments/chatbot-v1/test.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m         conv1\u001b[39m.\u001b[39mappend((\u001b[39m\"\u001b[39m\u001b[39muser_1\u001b[39m\u001b[39m\"\u001b[39m, next_input))\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f707974686f6e2d746f7263682d636f6e7461696e6572227d/home/Documents/Coding/python/ai/llama-experiments/chatbot-v1/test.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m         \u001b[39mprint\u001b[39m(conv1[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py:1202\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1200\u001b[0m     msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1201\u001b[0m     \u001b[39mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[0;32m-> 1202\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_input_request(\n\u001b[1;32m   1203\u001b[0m     \u001b[39mstr\u001b[39;49m(prompt),\n\u001b[1;32m   1204\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_parent_ident[\u001b[39m\"\u001b[39;49m\u001b[39mshell\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m   1205\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_parent(\u001b[39m\"\u001b[39;49m\u001b[39mshell\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1206\u001b[0m     password\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m   1207\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py:1245\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1242\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1243\u001b[0m     \u001b[39m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m   1244\u001b[0m     msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mInterrupted by user\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 1245\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m(msg) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1246\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m   1247\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog\u001b[39m.\u001b[39mwarning(\u001b[39m\"\u001b[39m\u001b[39mInvalid Message:\u001b[39m\u001b[39m\"\u001b[39m, exc_info\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "# conv1 is a list of turns in the conversation. Each turn is a tuple of (speaker, text)\n",
    "conv1 = []\n",
    "# Create the prompt for the conversation. This will be somthing like \"Below is a conversation:\"\n",
    "prompt = formatting.get_chat_prompt([\"user_1\", \"user_2\"], [\"Josh\", \"Mr Mainframe\"])\n",
    "\n",
    "# Print out the conversation up to this point\n",
    "print(prompt)\n",
    "for t in conv1:\n",
    "    print(t)\n",
    "\n",
    "# Generate 10 more turns. The AI will generate text for both speakers in the conversation, following the initial turn.\n",
    "for i in range(100):\n",
    "    if len(conv1)==0 or conv1[-1][0] == \"user_2\":\n",
    "        next_input = input(\"user_1: \")\n",
    "        conv1.append((\"user_1\", next_input))\n",
    "        print(conv1[-1])\n",
    "    else:\n",
    "        # Tokenize the conversation up to this point\n",
    "        model_input = formatting.tokenize_with_turn_trucation(tokenizer, prompt, conv1, next_turn=\"user_2\" if conv1[-1][0] == \"user_1\" else \"user_1\", for_inference=True)\n",
    "        # Generate the next turn. This ouputs not just the ai generated text, but also all the text that was inputted (with nex new txt on the end)\n",
    "        with torch.no_grad():\n",
    "            generated = model.generate(**model_input, max_new_tokens=50, num_beams=1, do_sample=True, temperature=1.0)[0]\n",
    "            model_output = tokenizer.decode(generated)\n",
    "        # Parse the model output to get the speaker and what they said\n",
    "        turns = model_output.split(\"</s>\")\n",
    "        if turns[-1] == \"\":\n",
    "            turns = turns[:-1]\n",
    "        new_turn = turns[-1].strip()\n",
    "        new_turn_parts = new_turn.split(\": \", 1)\n",
    "        # Add the new turn to the conversation\n",
    "        conv1.append((new_turn_parts[0], new_turn_parts[1]))\n",
    "        # Print out the conversation up to this point\n",
    "        print(conv1[-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
